all_vcfs = [expand('data/auxiliary/discovery/freebayes/{method}/{run}/' + str(config["pangenomeCorrectionK"]) + '/{barcode}.vcf', method=methods, barcode=barcodes[run], run=run) for run in runs]
#all_vcfs += [expand('data/auxiliary/discovery/cortex/{method}/{run}/' + str(config["pangenomeCorrectionK"]) + '/{barcode}.vcf', method=methods, barcode=barcodes[run], run=run) for run in runs]

#Although we don't use nanopolish READS we do use the nanopolish vcfs
allMethods = ['medaka','nanopolish']

all_vcfs += [expand('data/input/{run}/barcode{barcode}.{method}.'+vcf_suffix, method=allMethods, barcode=barcodes[run], run=run) for run in runs]

vcfs = [item for sublist in all_vcfs for item in sublist]

#Add Pangenome VCF
vcfs += ['data/input/' + config['gisaidVCF']]

rule concatenate_vcf:
    input:
        vcfs = lambda wlc: vcfs,
    output:
        out = "data/auxiliary/pangenome/merge.vcf"
    conda:
        '../envs/freebayes.yaml',
    threads:
        48
    log:
        'logs/pangenome_concatenate_vcf.log'
    script:
        '../scripts/pangenome/concate_vcf.py'

rule hackfix:
    input:
        "data/auxiliary/pangenome/merge.vcf"
    output:
        "data/auxiliary/pangenome/merge_fixed.vcf"
    shell:
        "sed 's/FORMAT\tall_var/FORMAT/' {input} > {output}"

rule sortVCF:
    input:
        file = "data/auxiliary/pangenome/merge_fixed.vcf"
    output:
        "data/auxiliary/pangenome/merge_sorted.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt sort {input.file} -o {output}'

rule decompose1:
    input:
        "data/auxiliary/pangenome/merge_sorted.vcf"
    output:
        "data/auxiliary/pangenome/merge_dc1.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt decompose {input} -o {output}'

rule decompose2:
    input:
        "data/auxiliary/pangenome/merge_dc1.vcf"
    output:
        "data/auxiliary/pangenome/merge_dc2.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt decompose_blocksub -a {input} -o {output}'

rule dropDuplicates:
    input:
        "data/auxiliary/pangenome/merge_dc2.vcf"
    output:
        "data/auxiliary/pangenome/merge_unique.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt uniq {input} -o {output}'

'''
rule sortVCF:
    input:
        "data/auxiliary/pangenome/merge.vcf"
    output:
        "data/auxiliary/pangenome/merge_sorted.vcf"
    shell:
        "cat {input} | awk \'$1 ~ /^#/ {{print $0;next}} {{print $0 | \"sort -k1,1 -k2,2n\"}}\' > {output}"
'''

rule bgzip:
    input:
        "data/auxiliary/pangenome/merge_unique.vcf"
    output:
        "data/auxiliary/pangenome/merge_unique.vcf.gz"
    conda:
        '../envs/tabix.yaml',

    shell:
        "bgzip {input}"

rule indexVcf:
    input:
        "data/auxiliary/pangenome/merge_unique.vcf.gz"
    output:
        "data/auxiliary/pangenome/merge_unique.vcf.gz.tbi"
    conda:
        '../envs/tabix.yaml',

    shell:
        "tabix -f -p vcf {input}"
       
rule pangenome:
    input:
        vcf = "data/auxiliary/pangenome/merge_unique.vcf.gz",
        reference = "data/input/nCoV-2019.reference.fasta",
        idx ="data/auxiliary/pangenome/merge_unique.vcf.gz.tbi"
    output:
        asm = "data/auxiliary/pangenome/pangenome_messy.gfa",

    conda:
        '../envs/vg.yaml',

    threads:
        48

    log:
        'logs/pangenome_build_pangenome.log'

    shell:
        "vg construct -t {threads} -S -i -r {input.reference} -v {input.vcf} | vg view -g - > {output}"

rule unchop:
    input:
        "data/auxiliary/pangenome/pangenome_messy.gfa",
    output:
        "data/auxiliary/pangenome/pangenome.gfa"
    conda:
        "../envs/vg.yaml"
    shell:
        "vg mod --unchop {input} > {output}"