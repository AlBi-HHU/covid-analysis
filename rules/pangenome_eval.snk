import random

#Should only be called on demand and not added to the SF as a default target file
rule generateTestSet:
    output:
        testSet = 'data/output/testSet.txt',
        evalSet = 'data/output/evaluationSet.txt'
    run:
        samples = []
        for run in runs:
            for barcode in barcodes[run]:
                samples.append((run,barcode))
        test = random.sample(samples,k=30)
        eval = set(samples)-set(test)
        with open(output['testSet'],'w') as testSet, open(output['evalSet'],'w') as evalSet:
            for t in test:
                testSet.write('{}\t{}\n'.format(t[0],t[1]))
            for e in eval:
                evalSet.write('{}\t{}\n'.format(e[0],e[1]))


rule combineWithReference_pancov:
    input:
        reference = "data/input/nCoV-2019.reference.fasta",
        consensus = "data/output/consensus/medaka/{run}/{barcode}/consensus.fasta"
    output:
        combined = "data/auxiliary/evaluation/consensusVariantExtraction/pancov/{run}_{barcode}.cmb"
    shell:
        'cat {input.reference} {input.consensus} > {output.combined}'

rule combineWithReference_nanomeda:
    input:
        reference = "data/input/nCoV-2019.reference.fasta",
        consensus = "data/input/{run}/barcode{barcode}.{method}.consensus.fasta"
    output:
        combined = "data/auxiliary/evaluation/consensusVariantExtraction/{method}/{run}_{barcode}.cmb"
    shell:
        'cat {input.reference} {input.consensus} > {output.combined}'


rule combineWithReference_gisaid:
    input:
        reference = "data/input/nCoV-2019.reference.fasta",
        consensus = lambda wildcards : "data/input/gisaidseqs/Germany_NW-HHU-"+getGisaidFile(wildcards.run,wildcards.barcode)+".fasta"
    output:
        combined = "data/auxiliary/evaluation/consensusVariantExtraction/gisaid/{run}_{barcode}.cmb"
    shell:
        'cat {input.reference} {input.consensus} > {output.combined}'

rule muscle:
    input:
        "data/auxiliary/evaluation/consensusVariantExtraction/{method}/{run}_{barcode}.cmb"
    output:
        "data/auxiliary/evaluation/consensusVariantExtraction/{method}/{run}_{barcode}.aln"
    conda:
        "../envs/muscle.yaml"
    shell:
        'muscle -in {input} -clwout {output}'


rule createInfoFile:
    input:
        alignment = "data/auxiliary/evaluation/consensusVariantExtraction/{method}/{run}_{barcode}.aln"
    output:
        info = "data/auxiliary/evaluation/consensusVariantExtraction/{method}/{run}_{barcode}.info"
    script:
        '../scripts/consensusToTable.py'

def getAllComparisonFiles(wildcards):
    allFiles = []
    for run in runs:
        for barcode in barcodes[run]:
            #check for missing gisaid files
            if wildcards.method == 'gisaid':
                if run in gisaidMapping and barcode in gisaidMapping[run]:
                    pass
                else:
                    print('skipping run {} barcode {} for gisaid comparison as we have no seq yet ...'.format(run,barcode))
                    continue

            allFiles += [
                "data/auxiliary/evaluation/consensusVariantExtraction/pancov/"+run+"_"+barcode+".info",
                "data/auxiliary/evaluation/consensusVariantExtraction/{method}/"+run+"_"+barcode+".info",
                'data/auxiliary/pileupAnalysis/medaka/' + run + '/' + barcode + '.pileupanalysis.txt'
            ]
    return allFiles

rule assembleMedians:
    input:
        fetchAllPileups()
    output:
        'data/auxiliary/pileupAnalysis/medians.json'
    script:
        '../scripts/assembleMedians.py'

rule comparePancovToX:
    input:
        iteratorList = getAllComparisonFiles,
        medians = 'data/auxiliary/pileupAnalysis/medians.json'
    output:
        'data/output/evaluation/comparisonFastaBased/{method}.eval'
    script:
        '../scripts/variantdiff_summary.py'

rule hackfix_eval_md:
    input:
        'data/input/{run}/barcode{barcode}.medaka.' + vcf_suffix
    output:
        "data/auxiliary/evaluation/vcfs/medaka_{run}_{barcode}.vcf"
    priority: 50
    shell:
        "sed 's/FORMAT\tall_var/FORMAT/' {input} | sed '3i ##contig=<ID=MN908947.3,length=29903>' > {output}"

rule hackfix_eval_np:
    input:
        'data/input/{run}/barcode{barcode}.nanopolish.' + vcf_suffix
    output:
        "data/auxiliary/evaluation/vcfs/nanopolish_{run}_{barcode}.vcf"
    priority: 50
    shell:
        "sed 's/FORMAT\tall_var/FORMAT/' {input} | sed '3i ##contig=<ID=MN908947.3,length=29903>' > {output}"

rule hackfix_eval_fb:
    input:
        'data/auxiliary/discovery/freebayes/medaka/{run}/' + str(config["pangenomeCorrectionK"]) + '/{barcode}.vcf',  # freebayes
    output:
        "data/auxiliary/evaluation/vcfs/freebayes_{run}_{barcode}.vcf"
    priority: 50
    shell:
        "sed 's/FORMAT\tall_var/FORMAT/' {input} > {output}"


rule sortVCF_eval:
    input:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.vcf",
    output:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.sorted.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt sort {input} -o {output}'

rule decompose1_eval:
    input:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.sorted.vcf"
    output:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.dc1.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt decompose {input} -o {output}'

rule decompose2_eval:
    input:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.dc1.vcf"
    output:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.dc2.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt decompose_blocksub -a {input} -o {output}'

rule dropDuplicates_eval:
    input:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.dc2.vcf"
    output:
        "data/auxiliary/evaluation/vcfs/{method}_{run}_{barcode}.unique.vcf"
    conda:
        '../envs/vt.yaml'
    shell:
        'vt uniq {input} -o {output}'


def getAllVCFsForContributions(wildcards):
    allFiles = []
    for run in runs:
        for barcode in barcodes[run]:
            #Skip distinguishing between medaka and nanopolish as separate samples here
            allFiles += [
                "data/auxiliary/pangenome_vc/medaka/"+run+"/"+barcode+"/filter.vcf", #pancov
                "data/auxiliary/evaluation/vcfs/freebayes_"+run+"_"+barcode+".unique.vcf", #freebayes
                "data/auxiliary/evaluation/vcfs/medaka_"+run+"_"+barcode+".unique.vcf", #medaka
                "data/auxiliary/evaluation/vcfs/nanopolish_"+run+"_"+barcode+".unique.vcf", #nanopolish
            ]
    return allFiles

rule getContributions:
    input:
        getAllVCFsForContributions
    output:
        'data/output/evaluation/contributions.txt',
        'data/output/evaluation/contributions_upset.svg'
    conda:
        '../envs/vcfpy.yaml'
    script:
        '../scripts/getContributionShares.py'

'''
import json

all_reads_eval = []
for run in runs:
    all_reads_eval += expand('data/auxiliary/corrections/{method}/'+run+'/21/{barcode}.fasta',method=methods,barcode=barcodes[run])

#Randomly samples subsets and saves them to disk
checkpoint createSubsets:
    input:
        all_reads_eval
    output:
        'data/auxiliary/pangenomeEval/sampleSets.json'
    script:
        '../scripts/createSubsets.py'

def subset(subsetFile,wildcards):
    sampleSets = json.load(open(subsetFile,'r'))
    ret = sampleSets[wildcards.cohortSize][int(wildcards.sampleSet)]
    print(ret)
    return ret

k = config["pangenomeKSize"]

rule count_kmer_of_all_files:
    input:
        subsetFile = 'data/auxiliary/pangenomeEval/sampleSets.json',
        reference = "data/input/nCoV-2019.reference.fasta"
    output:
        count = "data/auxiliary/pangenome/{cohortSize}/{sampleSet}/all_reads_and_ref.h5"
    params:
        k = k,
        abundance_min = lambda wildcards,input : ','.join([str(config["pangenomeMinCov"]) for _ in range(len(subset(input.subsetFile,wildcards)))]),
        readss = lambda wildcards,input : ','.join(subset(input.subsetFile,wildcards))
    conda:
        '../envs/dsk.yaml',
    threads:
        48
    log:
        'logs/{cohortSize}/{sampleSet}/pangenome_dsk.log'
    shell:
        'dsk -file {params.readss},{input.reference},{input.reference} -kmer-size {params.k} -abundance-min {params.abundance_min},1,1 -solidity-kind one -repartition-type 1 -minimizer-type 1 -nb-cores {threads} -out {output.count} 2> {log}'

        
rule asm_pangenome:
    input:
        count = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.h5",

    output:
        asm = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.unitigs.fa",
        
    params:
        outprefix = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref",
        k = k,

    conda:
        '../envs/bcalm.yaml',

    threads:
        48

    log:
        'logs/{cohortSize}/{sampling}/pangenome_bcalm.log'

    shell:
        'bcalm -in {input.count} -kmer-size {params.k} -repartition-type 1 -minimizer-type 1 -nb-cores {threads} -out {params.outprefix} 2> {log}'


rule generate_gfa_pangenome:
    input:
        asm = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.unitigs.fa",
        
    output:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        
    params:
        k = k,
        
    log:
        'logs/{cohortSize}/{sampling}/pangenome_asm2gfa.log'

    shell:
        'python3 scripts/convertToGFA.py {input.asm} {output.graph} {params.k} 2> {log}'

rule map_read_on_graph:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        subsetFile = 'data/auxiliary/pangenomeEval/sampleSets.json',
        readss = lambda wildcards : subset(input.subsetFile,wildcards)

    output:
        mapping = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gaf",

    conda:
        '../envs/graphaligner.yaml',

    threads:
        48

    log:
        'logs/{cohortSize}/{sampling}/pangenome_mapping.log'

    shell:
        "GraphAligner -t {threads} -g {input.graph} -f {input.readss} -a {output.mapping}"


rule add_reads_path:
    input:
        graph = "{prefix}.gfa",
        mapping = "{prefix}.gaf",

    output:
        out_graph = "{prefix}.paths.gfa",
        
    params:
        min_mapping_coverage = 150,
        prefix = "p",
        
    log:
        'logs/add_gaf_gfa_{prefix}.log'
        
    script:
        '../scripts/add_gaf_gfa.py'

rule map_reference_on_graph:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        reference = "data/input/nCoV-2019.reference.fasta"

    output:
        mapping = "data/auxiliary/{cohortSize}/{sampling}/pangenome/all_reads_and_ref.reference.gaf",

    conda:
        '../envs/graphaligner.yaml',

    threads:
        48

    log:
        'logs/{cohortSize}/{sampling}/pangenome_ref_mapping.log'

    shell:
        "GraphAligner -t {threads} -g {input.graph} -f {input.reference} -a {output.mapping}"

        
rule add_reference_path:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.paths.gfa",
        mapping = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.reference.gaf",

    output:
        out_graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.paths.ref.gfa",
        
    params:
        min_mapping_coverage = 0,
        prefix = "ref",
        
    log:
        'logs/{cohortSize}/{sampling}/add_gaf_gfa_reference.log'
        
    script:
        '../scripts/add_gaf_gfa.py'

########### VC ##############

rule map_read_pangenome:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        reads = "data/auxiliary/softClippedSeqs/{method}/{run}/{barcode}.fasta",
    output:
        mappings = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/reads.gaf",

    conda:
        '../envs/graphaligner.yaml',

    threads:
        48
                
    log:
        'logs/{cohortSize}/{sampling}/pangenome_vc_read_mapping_{method}_{run}_{barcode}.log',

    shell:
        "GraphAligner -t {threads} -g {input.graph} -f {input.reads} -a {output.mappings} 2> {log}"


rule map_ref_pangenome:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        ref = "data/input/nCoV-2019.reference.fasta",
        
    output:
        mappings = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/reference.gaf",

    conda:
        '../envs/graphaligner.yaml',

    threads:
        48
                
    log:
        'logs/{cohortSize}/{sampling}/pangenome_vc_reference_mapping.log',

    shell:
        "GraphAligner -t {threads} -g {input.graph} -f {input.ref} -a {output.mappings} 2> {log}"


rule call_variant:
    input:
        graph = "data/auxiliary/pangenome/{cohortSize}/{sampling}/all_reads_and_ref.gfa",
        reads_mapping = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/reads.gaf",
        reference_mapping = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/reference.gaf",
        
    output:
        variant = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/variant_ugly.vcf",
        node_pos_on_ref = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/node2pos.csv"

    params:
        ksize = config["pangenomeKSize"],
        rvt_threshold = config["pangenomeRVTThreshold"]
        
    log:
        'logs/{cohortSize}/{sampling}/pangenome_vc_call_variant_{method}_{run}_{barcode}.log',

    script:
        "../scripts/call_variant_pangenome.py"


rule normalize_variant:
    input:
        ugly_variant = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/variant_ugly.vcf",
        reference = "data/input/nCoV-2019.reference.fasta",

    output:
        variant = "data/auxiliary/pangenome_vc/{cohortSize}/{sampling}/{method}/{run}/{barcode}/variant.vcf",

    conda:
        '../envs/vt.yaml',

    log:
        'logs/{cohortSize}/{sampling}/pangenome_vc_normalize_variant_{method}_{run}_{barcode}.log'

    shell:
        'vt normalize -r {input.reference} -o {output.variant} {input.ugly_variant} 2> {log}'
'''
